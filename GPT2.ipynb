{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f2d32d-a2a5-4d07-82a8-a75f9d59a5c2",
   "metadata": {},
   "source": [
    "## Translating NanoGPT (GPT2) to TensorFlow\n",
    "\n",
    "#### Based on https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd63294-aad8-43f0-af26-fd269065c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from  dataclasses import dataclass\n",
    "from tensorflow.experimental import numpy as tnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e066ce-3b7f-4b67-9baf-0447689759ca",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1815235f-5c52-47f2-b907-d76302440ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNorm(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, bias=True, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.bias = bias\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.weight = self.add_weight(name='weight',\n",
    "                                      shape=input_shape[-1:], # [-1:] gives last elem but keeps dims\n",
    "                                      initializer=tf.keras.initializers.Ones(),\n",
    "                                      trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                      shape=input_shape[-1:], # [-1:] gives last elem but keeps dims\n",
    "                                      initializer=tf.keras.initializers.Zeros(),\n",
    "                                      trainable=True) if self.bias else None\n",
    "\n",
    "        super(MyLayerNorm, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Can also use tf.nn.moments(inputs, axes=-1, keepdims=True), \n",
    "        # but then additionally one needs to take the sqrt to get \\sigma\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        return self.weight * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30015f0-edcb-4f58-95ba-7ef0b1f9b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 8 # 1024 for GPT2\n",
    "    vocab_size: int = 20 # 50304 for GPT2\n",
    "    n_layer: int = 2 # 12\n",
    "    n_head: int = 2 # 12\n",
    "    n_embd: int = 10 # 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "    seed: int = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62748a28-fe5e-447a-9ee0-364f628721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"Embedding dimension must divide number of heads\"\n",
    "        # key, query, value computed at once and splitted later\n",
    "        self.initializer_proj = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02 / tf.math.sqrt(2. * config.n_layer), seed=None)\n",
    "        self.c_attn = tf.keras.layers.Dense(#config.n_embd,\n",
    "                                            3 * config.n_embd,\n",
    "                                            activation=None,\n",
    "                                            use_bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = tf.keras.layers.Dense(#config.n_embd,\n",
    "                                            config.n_embd,\n",
    "                                            activation=None,\n",
    "                                            kernel_initializer=self.initializer_proj,\n",
    "                                            use_bias=config.bias)\n",
    "        self.dropout = config.dropout\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.resid_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.mask = tf.experimental.numpy.tril(\n",
    "            tf.ones([config.block_size, config.block_size]))[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, C = x.size() # batch, sequence and channel, which is the embedding dim\n",
    "\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, axis=2)\n",
    "        k = tf.transpose(tf.reshape(k, [B, T, self.n_head, C // self.n_head]),\n",
    "                         perm=[0, 2, 1, 3])\n",
    "        q = tf.transpose(tf.reshape(q, [B, T, self.n_head, C // self.n_head]),\n",
    "                         perm=[0, 2, 1, 3])\n",
    "        v = tf.transpose(tf.reshape(v, [B, T, self.n_head, C // self.n_head]),\n",
    "                         perm=[0, 2, 1, 3])\n",
    "\n",
    "        att = (q @ tf.transpose(k, perm=[0, 1, 3, 2])) * (1.0 / tf.math.sqrt(k.shape[-1]))\n",
    "\n",
    "        mask = tf.experimental.numpy.tril(tf.ones([T, T]))[tf.newaxis, tf.newaxis, :, :]\n",
    "        att = tf.where(mask != 0, att, tf.constant(-np.inf))\n",
    "        att = tf.nn.softmax(att, axis = 3)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "\n",
    "        y = tf.reshape(tf.transpose(y, perm=[0, 2, 1, 3]), [B, T, C])\n",
    "\n",
    "        return self.resid_dropout(self.c_proj(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f03ff7b-782a-44ce-90d6-740bea8696d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.initializer_proj = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02 / tf.math.sqrt(2. * config.n_layer), seed=None)\n",
    "        # Streching and shrinking in channel/embedding dimension,\n",
    "        # like for large resnets\n",
    "        self.c_fc = tf.keras.layers.Dense(4 * config.n_embd, activation=None, use_bias=config.bias)\n",
    "        self.c_proj = tf.keras.layers.Dense(config.n_embd, activation=None, kernel_initializer=self.initializer_proj, use_bias=config.bias)\n",
    "        self.gelu = tf.keras.activations.gelu\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b9f2e7-a6ca-42f1-a4f1-9fd4c70a5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = MyLayerNorm(bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = MyLayerNorm(bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        return x + self.mlp(self.ln_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c27098-de9a-4d27-81dd-03252d470f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.initializer_dense = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=self.config.seed)\n",
    "        self.initializer_embed = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=self.config.seed)\n",
    "        self.initializer_bias = tf.keras.initializers.Zeros()\n",
    "        \n",
    "        self.wte = tf.keras.layers.Embedding(self.config.vocab_size, self.config.n_embd, \n",
    "                                             embeddings_initializer=self.initializer_embed, name='wte')\n",
    "        self.wpe = tf.keras.layers.Embedding(self.config.vocab_size, self.config.n_embd, \n",
    "                                             embeddings_initializer=self.initializer_embed, name='wpe')\n",
    "        self.drop = tf.keras.layers.Dropout(self.config.dropout, name='drop')\n",
    "        self.h = [Block(self.config) for _ in range(self.config.n_layer)]\n",
    "        self.ln_f = MyLayerNorm(bias=self.config.bias, name='ln_f')\n",
    "\n",
    "        self.lm_head = tf.keras.layers.Dense(self.config.vocab_size, use_bias=self.config.bias)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.wte.build(input_shape=[self.config.vocab_size])\n",
    "        self.lm_head.build(input_shape=[self.config.n_embd])\n",
    "        self.wte.trainable_weights[0].assign(tf.transpose(self.lm_head.trainable_weights[0]))\n",
    "        \n",
    "    def call(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        assert t <= self.config.block_size, f'sequence too long for the defined context of {self.config.block_size}'\n",
    "        pos = tf.range(0, t, dtype=tf.int64)\n",
    "\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            #print(tf.reshape(targets, [-1]).shape)\n",
    "            #print(tf.reshape(logits, [-1, logits.shape[-1]]).shape)\n",
    "            \n",
    "            loss = ce(tf.reshape(targets, [-1]),\n",
    "                      tf.reshape(logits, [-1, logits.shape[-1]]))\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, -1, :])[:, tf.newaxis, :]\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918718bb-997f-4470-8cfe-bd17a1539eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4b91fa-8fdd-4472-b930-2be16cc4370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=8, vocab_size=20, n_layer=2, n_head=2, n_embd=10, dropout=0.0, bias=True, seed=1337)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bab698-a256-4958-a02f-97e9af6e3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = tf.constant(np.random.randint(0, 9, size=[2, 8]), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081ea851-b392-418b-9920-7a193dc32757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       "array([[1, 1, 7, 4, 2, 1, 8, 3],\n",
       "       [0, 7, 0, 7, 6, 7, 3, 3]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06239357-d63e-4851-b9fd-f6709f73690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc303292-68b9-4094-8cd8-352bb0b0ca33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 20), dtype=float32, numpy=\n",
       " array([[[-4.89144444e-01,  2.86876678e+00, -7.82703698e-01,\n",
       "          -9.35716271e-01,  6.72218502e-01, -1.04115403e+00,\n",
       "           1.04174525e-01,  4.72361207e-01,  2.62994796e-01,\n",
       "           7.24863529e-01,  1.14319062e+00, -1.20986927e+00,\n",
       "           5.66032052e-01,  1.41744149e+00, -4.50987160e-01,\n",
       "          -4.88783598e-01, -2.80468225e-01,  5.11749923e-01,\n",
       "          -7.99474001e-01,  8.02277982e-01],\n",
       "         [-4.80725348e-01,  2.86760473e+00, -8.07274878e-01,\n",
       "          -9.12304997e-01,  6.47316635e-01, -1.13246274e+00,\n",
       "           1.05220005e-01,  4.77306843e-01,  2.71067053e-01,\n",
       "           7.18799889e-01,  1.09386218e+00, -1.16527736e+00,\n",
       "           6.16618156e-01,  1.40563095e+00, -4.27604437e-01,\n",
       "          -5.55797338e-01, -2.67343968e-01,  5.24828434e-01,\n",
       "          -8.71297061e-01,  8.63418519e-01],\n",
       "         [ 4.81840849e-01,  6.94311261e-01, -3.77012879e-01,\n",
       "          -1.82848072e+00,  1.37807798e+00, -4.80995744e-01,\n",
       "           1.02058434e+00,  1.84461892e+00, -9.02379990e-01,\n",
       "          -3.59456986e-01,  2.28398263e-01, -1.44533920e+00,\n",
       "           5.57998896e-01,  3.41550320e-01,  2.84916341e-01,\n",
       "          -4.97604370e-01, -6.79164767e-01, -5.52147806e-01,\n",
       "           4.32242930e-01,  4.74818349e-01],\n",
       "         [ 1.02249481e-01,  7.40127385e-01, -1.15196836e+00,\n",
       "          -1.94125676e+00,  1.84465480e+00,  5.25900364e-01,\n",
       "           2.03707054e-01,  1.33119953e+00, -2.16699556e-01,\n",
       "          -1.41494572e-01,  7.07335830e-01, -6.78570390e-01,\n",
       "          -3.80089670e-01,  1.21184671e+00, -3.40304375e-02,\n",
       "           9.40185666e-01, -4.24073875e-01, -2.09090054e-01,\n",
       "           1.03701997e+00, -8.51403832e-01],\n",
       "         [ 5.95015436e-02, -8.27918530e-01,  2.89479136e+00,\n",
       "           8.11271071e-01, -7.77526379e-01,  7.44982183e-01,\n",
       "          -2.22527757e-02, -2.57731527e-01,  3.03284526e-02,\n",
       "          -9.99990255e-02, -3.38908881e-01, -5.68812430e-01,\n",
       "          -1.97114050e-02, -1.34101200e+00, -1.68171525e-02,\n",
       "          -4.67181504e-01,  6.05433285e-01, -1.93975866e-01,\n",
       "           5.49857259e-01,  9.28856134e-01],\n",
       "         [-5.08797705e-01,  2.86749315e+00, -9.00373578e-01,\n",
       "          -8.69451284e-01,  6.02967441e-01, -1.07453203e+00,\n",
       "           1.01456344e-01,  3.84239137e-01,  2.96318889e-01,\n",
       "           7.67472625e-01,  1.14498091e+00, -1.13021398e+00,\n",
       "           5.69370389e-01,  1.41581416e+00, -4.59931135e-01,\n",
       "          -4.79695916e-01, -3.05192411e-01,  5.24547219e-01,\n",
       "          -8.54718685e-01,  7.48701990e-01],\n",
       "         [-7.68246770e-01,  3.45267832e-01,  1.17108434e-01,\n",
       "           4.77049619e-01, -1.49059817e-01, -2.71559954e-02,\n",
       "          -5.41039467e-01, -6.57638073e-01,  2.41018796e+00,\n",
       "           1.41929483e+00, -3.47512901e-01,  1.34260929e+00,\n",
       "          -2.03136027e-01, -1.45929456e-01,  1.45627409e-01,\n",
       "           4.86581683e-01,  7.87063122e-01,  4.16386276e-01,\n",
       "           3.28662336e-01, -1.72487035e-01],\n",
       "         [-7.81895757e-01, -9.80891824e-01,  8.92719388e-01,\n",
       "           2.65693331e+00, -1.33865070e+00, -3.96957338e-01,\n",
       "          -4.87112820e-01, -1.22682583e+00,  3.99192452e-01,\n",
       "           3.01312029e-01, -9.25901592e-01,  9.27478194e-01,\n",
       "          -1.82354599e-01, -7.01102853e-01,  4.77708429e-01,\n",
       "          -2.71340966e-01,  6.27372146e-01,  1.44943714e-01,\n",
       "          -9.25434649e-01,  7.96198100e-03]],\n",
       " \n",
       "        [[ 1.63698173e+00, -9.35669124e-01,  1.83850199e-01,\n",
       "          -1.32540178e+00,  1.06990285e-01,  3.79484206e-01,\n",
       "           2.37705022e-01,  6.55298412e-01, -1.12082005e+00,\n",
       "          -8.30780089e-01, -7.21893132e-01, -5.90481877e-01,\n",
       "          -2.38232464e-01, -1.02254212e+00,  5.87939382e-01,\n",
       "           9.15443525e-04,  1.89410746e-02,  3.50887448e-01,\n",
       "           3.57075870e-01,  4.74477977e-01],\n",
       "         [ 5.45206547e-01,  5.40017366e-01, -3.73827159e-01,\n",
       "          -1.70565152e+00,  1.28650939e+00, -5.89353800e-01,\n",
       "           1.03148115e+00,  1.85398495e+00, -8.95945847e-01,\n",
       "          -5.13788700e-01, -4.65055630e-02, -1.40460777e+00,\n",
       "           5.07393241e-01,  2.76699245e-01,  3.52886349e-01,\n",
       "          -5.79191506e-01, -6.98827028e-01, -5.64934611e-01,\n",
       "           2.56087631e-01,  6.12852216e-01],\n",
       "         [ 1.62633359e+00, -8.66725922e-01,  1.82390124e-01,\n",
       "          -1.34021056e+00,  4.99558598e-02,  4.17757094e-01,\n",
       "           1.42558649e-01,  5.15648901e-01, -1.09486496e+00,\n",
       "          -6.67475343e-01, -5.36911428e-01, -4.60950971e-01,\n",
       "          -1.54450566e-01, -1.05597639e+00,  5.53438246e-01,\n",
       "           2.92499792e-02,  1.31961673e-01,  4.65194255e-01,\n",
       "           4.23432648e-01,  3.98797929e-01],\n",
       "         [ 5.83029687e-01,  3.85310411e-01, -3.12694103e-01,\n",
       "          -1.75283051e+00,  1.29478788e+00, -4.04446959e-01,\n",
       "           1.07929611e+00,  1.84765756e+00, -9.10992980e-01,\n",
       "          -5.04911482e-01, -1.98083036e-02, -1.41009319e+00,\n",
       "           4.44862515e-01,  1.68415710e-01,  3.50663215e-01,\n",
       "          -4.74407196e-01, -7.27294385e-01, -6.38931394e-01,\n",
       "           4.54179138e-01,  4.76736158e-01],\n",
       "         [ 1.35764509e-01,  8.43583792e-03,  1.44203186e-01,\n",
       "          -6.51212633e-01,  1.29349113e-01, -2.08167881e-01,\n",
       "           1.87173378e+00,  1.01912642e+00, -7.03628957e-01,\n",
       "          -2.23479867e-01,  1.74779281e-01, -1.58787739e+00,\n",
       "           6.85367584e-01, -5.81561327e-01, -2.15402186e-01,\n",
       "          -9.22128439e-01, -1.51478171e+00, -1.55087626e+00,\n",
       "           4.47985113e-01,  3.54212075e-01],\n",
       "         [ 5.55603266e-01,  5.24269938e-01, -5.30612171e-01,\n",
       "          -1.75779080e+00,  1.30815423e+00, -5.24170935e-01,\n",
       "           1.10629809e+00,  1.84963560e+00, -9.34525311e-01,\n",
       "          -4.94830668e-01,  6.01259619e-03, -1.43938875e+00,\n",
       "           4.62682366e-01,  2.85708845e-01,  3.40206951e-01,\n",
       "          -4.97057408e-01, -8.07700336e-01, -6.21484697e-01,\n",
       "           3.20068359e-01,  4.67632860e-01],\n",
       "         [-8.05650830e-01, -9.30641532e-01,  9.66201544e-01,\n",
       "           2.65275621e+00, -1.30493295e+00, -3.55240345e-01,\n",
       "          -4.71022189e-01, -1.18327212e+00,  4.34750974e-01,\n",
       "           2.55379051e-01, -9.60687816e-01,  8.05685103e-01,\n",
       "          -2.62386799e-01, -6.53932393e-01,  4.51159924e-01,\n",
       "          -2.47457638e-01,  5.90385795e-01,  1.15890726e-01,\n",
       "          -9.28703427e-01,  5.63565530e-02],\n",
       "         [-7.81895757e-01, -9.80891824e-01,  8.92719388e-01,\n",
       "           2.65693331e+00, -1.33865070e+00, -3.96957338e-01,\n",
       "          -4.87112820e-01, -1.22682583e+00,  3.99192452e-01,\n",
       "           3.01312029e-01, -9.25901592e-01,  9.27478194e-01,\n",
       "          -1.82354599e-01, -7.01102853e-01,  4.77708429e-01,\n",
       "          -2.71340966e-01,  6.27372146e-01,  1.44943714e-01,\n",
       "          -9.25434649e-01,  7.96198100e-03]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2140949>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt(txt, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445c587-127f-4847-a376-83137d4872eb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9783bf-ffd8-47a7-a96d-7e565b2fd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespear_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file('shakespear.txt', shakespear_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b924d61-10ce-4a53-9bcf-988ac292bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    shakespear_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc58f79-1e89-4670-968d-b5ac2ddeaf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shakespear_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895d2e3f-a3b9-4206-8a42-9c8ef6878241",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split='character',\n",
    "                                                  standardize='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69510b33-0bc0-4949-8354-a3e46e6ce141",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer.adapt([shakespear_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb9ef03-8cb6-460e-86e1-0f4ef25bddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', ' ', 'e', 't', 'o', 'a', 'i', 'h', 's']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1790fb-ae48-4d91-8271-52dd1e9728e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = text_vec_layer([shakespear_txt])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a193190e-994c-4102-9641-9b6094057233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48b4bc16-834b-4dcb-bf3a-9dcf99a75cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing code 0 and 1 reserved for padding and unknown characters \n",
    "# (codes start at 2 before that removal so now 0 and 1 will be some chars)\n",
    "encoded -= 2\n",
    "\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f67da86c-ac61-4c4e-9a1d-df7ae567eed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ef9654-7a8f-46a7-bd8a-2c05f4c7eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42db9390-638c-4bc8-89a9-2fb3a8bf608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53836358-b36f-4325-ad67-69dfa6bd38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 8\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b70c83e-72b1-4449-87b5-c805e996e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = to_dataset(encoded[:1_060_000], length=length, shuffle=True, seed=1337)\n",
    "valid_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c389baff-d9d6-4592-8faa-1429d90f4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 3 14  1 26 10 10 25  5]], shape=(1, 8), dtype=int64)\n",
      "tf.Tensor([[14  1 26 10 10 25  5  8]], shape=(1, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for sample in train_set.rebatch(1).take(1):\n",
    "    print(sample[0])\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a7a825-e4ca-4684-9fef-d50ebc0a500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 2\n",
    "n_head = 2 \n",
    "n_embd = 32\n",
    "block_size = length\n",
    "bias = True\n",
    "vocab_size = n_tokens\n",
    "dropout = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "max_iters = 100\n",
    "\n",
    "init_from = 'scratch'\n",
    "\n",
    "weight_decay = tf.constant(1e-1, dtype=tf.float32)\n",
    "\n",
    "warmup_iters = 10\n",
    "learning_rate = tf.constant(6e-4, dtype=tf.float32)\n",
    "lr_decay_iters = 100 # == max_iters\n",
    "min_lr = tf.constant(6e-5, dtype=tf.float32)\n",
    "\n",
    "eval_iters = 20\n",
    "eval_interval = 10\n",
    "\n",
    "grad_clip = tf.constant(1.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0666b4cf-de36-46eb-b843-ba07a0dd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=n_layer, \n",
    "                  n_head=n_head, \n",
    "                  n_embd=n_embd,\n",
    "                  block_size=block_size,\n",
    "                  bias=bias,\n",
    "                  vocab_size=vocab_size,\n",
    "                  dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a18b3ce-bc94-444c-b924-1b07dae0d658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layer': 2,\n",
       " 'n_head': 2,\n",
       " 'n_embd': 32,\n",
       " 'block_size': 8,\n",
       " 'bias': True,\n",
       " 'vocab_size': 39,\n",
       " 'dropout': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a3b2d39-fdbc-4b83-8110-100c6c8e4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_from == 'scratch':\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49a20d00-713b-43cb-8eed-9254b62a960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, _ in train_set.take(1):\n",
    "    out = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c30241ec-6df0-4cc0-97f0-855235849fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 1, 39), dtype=float32, numpy=\n",
       " array([[[ 5.2112837 ,  2.0908573 , -1.7094367 , ..., -1.2486523 ,\n",
       "           0.69483757, -0.00625577]],\n",
       " \n",
       "        [[-0.26086673, -0.7746036 , -0.5939345 , ...,  0.542379  ,\n",
       "           0.046754  , -0.31683245]],\n",
       " \n",
       "        [[ 5.2112837 ,  2.0908573 , -1.7094367 , ..., -1.2486523 ,\n",
       "           0.69483757, -0.00625577]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.26086673, -0.7746036 , -0.5939345 , ...,  0.542379  ,\n",
       "           0.046754  , -0.31683245]],\n",
       " \n",
       "        [[-0.26086673, -0.7746036 , -0.5939345 , ...,  0.542379  ,\n",
       "           0.046754  , -0.31683245]],\n",
       " \n",
       "        [[ 5.2112837 ,  2.0908573 , -1.7094367 , ..., -1.2486523 ,\n",
       "           0.69483757, -0.00625577]]], dtype=float32)>,\n",
       " None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c998a4c-a0ad-4d19-a061-17a8b16f9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    learning_rate: tf.float32 = learning_rate\n",
    "    warmup_iters: int = warmup_iters\n",
    "    min_lr: tf.float32 = min_lr \n",
    "    lr_decay_iters: int= lr_decay_iters\n",
    "    \n",
    "    def __call__(self, step):\n",
    "      if step < self.warmup_iters:\n",
    "          return self.learning_rate * float(step) / self.warmup_iters\n",
    "      if step > self.lr_decay_iters:\n",
    "          return self.min_lr\n",
    "      decay_ratio = (float(step) - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)\n",
    "      assert 0 <= decay_ratio <= 1\n",
    "      coeff = 0.5 * (1.0 + tf.math.cos(tnp.pi * decay_ratio))\n",
    "      return self.min_lr + coeff * tf.cast((self.learning_rate - self.min_lr), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f42d01b-d266-4770-818f-b8a965ba65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=MyLRSchedule(learning_rate, warmup_iters, min_lr, lr_decay_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b8f304d-3e1b-423f-aeb6-2c58df083e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    out = {'train': None, 'val': None}\n",
    "    \n",
    "    losses = tf.zeros(eval_iters)\n",
    "    k = tf.Variable(0)\n",
    "    for X, Y in train_set.take(eval_iters):\n",
    "        logits, loss = model(X, Y, training=False)\n",
    "        tf.tensor_scatter_nd_update(losses, [[k]], [loss])\n",
    "        k.assign_add(1)\n",
    "    out['train'] = tf.reduce_mean(loss)\n",
    "\n",
    "    losses = tf.zeros(eval_iters)\n",
    "    k = tf.Variable(0)\n",
    "    for X, Y in valid_set.take(eval_iters):\n",
    "        logits, loss = model(X, Y, training=False)\n",
    "        tf.tensor_scatter_nd_update(losses, [[k]], [loss])\n",
    "        k.assign_add(1)\n",
    "    out['val'] = tf.reduce_mean(loss)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a9be03c-6bce-48ea-9314-2174491da0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <tf.Tensor: shape=(), dtype=float32, numpy=5.441515>,\n",
       " 'val': <tf.Tensor: shape=(), dtype=float32, numpy=5.13965>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2bef3f8-2226-43c8-a82b-8fa8c3e52f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_only = False\n",
    "eval_interval=1\n",
    "iter_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85bcc203-f181-4761-9215-3cbe90912eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: train loss 5.1932, val loss 5.1211\n",
      "step 6: train loss 5.2641, val loss 5.1120\n",
      "step 7: train loss 5.3812, val loss 5.1011\n",
      "step 8: train loss 5.3169, val loss 5.0886\n",
      "step 9: train loss 5.3550, val loss 5.0741\n",
      "step 10: train loss 5.2777, val loss 5.0579\n",
      "step 11: train loss 5.2650, val loss 5.0400\n",
      "step 12: train loss 5.2888, val loss 5.0226\n",
      "step 13: train loss 5.2873, val loss 5.0054\n",
      "step 14: train loss 5.1705, val loss 4.9881\n",
      "step 15: train loss 5.1544, val loss 4.9709\n",
      "step 16: train loss 5.1350, val loss 4.9537\n",
      "step 17: train loss 5.1135, val loss 4.9369\n",
      "step 18: train loss 5.0683, val loss 4.9201\n",
      "step 19: train loss 5.0741, val loss 4.9034\n",
      "step 20: train loss 5.0603, val loss 4.8868\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    for X, Y in train_set.take(1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, main_loss = model(X, Y, training=True)\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(g, \n",
    "                                      clip_value_min=-grad_clip, \n",
    "                                      clip_value_max=grad_clip) for g in gradients]\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))    \n",
    "    \n",
    "    iter_num += 1\n",
    "    if iter_num > 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99438fba-537b-4338-b9d5-70926839084e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
