#global_batch_size: batch_per_replica * strategy.num_replicas_in_sync
#buffer_size: 1024

globals:
  batch_per_replica: 512
  best_val_loss: 1e9
  weight_decay: 1e-1
  warmup_iters: 50
  learning_rate: 6e-4
  max_iters: 250
  lr_decay_iters: 250
  min_lr: 6e-5
  eval_iters: 20
  grad_clip: 1.0
  eval_only: False
  eval_interval: 1
  always_save_checkpoint: True
  restore: False
  checkpoint_directory: "./checkpoints"
  log_dir: "./tensorboard"

gpt:

  n_layer: 8
  n_head: 8
  n_embd: 32
  block_size: 32
  bias: True
  vocab_size: 39
  dropout: 0.1
  seed: 1337